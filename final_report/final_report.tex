\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tikz}

\author{Team Gamma \\ {\small Ajda Frankovič, Martin Preradović, Niki Bizjak}}
\title{Final report}
\date{}
\begin{document}
	
	\maketitle
	
	\section{Introduction}
	
	\section{Methods}
	In this section, the algorithms and methods we have used in our system are presented.
	
	\subsection{Camera pixel to world position}
	Object detectors must be able to compute world positions of detected objects. To do that, \texttt{rgb} camera and depth images are used. Because the robot is moving, and the environment is changing very fast, we must make sure that the received colour and depth image are synchronized with one another. Even a small delay between images can cause computed world positions to be inaccurate. \\
	
	\begin{figure}[h]
		\centering
		\caption{Pinhole camera model}
		\label{fig:pinhole_camera_model}
	\end{figure}
	
	When object is detected on \texttt{rgb} colour image, the pixel position on image plane is known. Using camera calibration matrix, we can obtain information about focal length and image format. Focal length is the distance from the centre of camera coordinate system to image plane. The image format gives us information about camera sensor width and height. \\
	
	Using this information, the ray from the centre of camera coordinate system to the detected object pixel on image plane can be created. Using depth image information, we can obtain the distance from camera centre to the detected object in space. Using the computed ray and distance to the object, we can compute its position in three-dimensional space in camera frame. \\
	
	Using frame transformations, we can then compute the detected object position in world coordinate system using transformation matrix. \\
	
	In our system, the face and ring positions are computed this way. The cylinder detection works with point clouds, where the points are already computed in the world frame.
	
	\subsection{Faces}
	In this section, we present algorithms that we have used to detect faces, compute their orientation in space and classify them.
	
	\subsubsection{Face detection}
	
	Face detection was done using Viola-Jones face detection algorithm. It was chosen because it can be run in real-time and it has a high detection rate. \\
	
	% Add more information about the Viola-Jones algorithm
	%   * explain haar cascade
	%   * explain why it works well
	%   * explain that it does not work well with images that are under different viewing angle and explaing that space exploration algorithm is used to compute points where each point of the map can be seen.
	
	% Add example images of Haar features
	
	% The Viola-Jones algorithm is performed with sliding window, explain that that takes time, so the image must be resized to get 30fps performance.
	
	\subsubsection{Face orientation computation}
	After the face has been detected in image, the position in world coordinate system is computed. The approaching point is then calculated using static map information. Approaching point is a point directly in front of the face that the robot must visit. \\
	
	To compute approaching point, we must first get the face orientation. To do that, the static map information is used. The calculated face position in world coordinates is first converted to map coordinates. Then, the corresponding pixel in map is calculated. \\
	
	\begin{figure}[h]
		% This image should be replaced with something better
		\centering
		\includegraphics[width=4cm]{images/orientation_computation}
		\caption{Face orientation computation}
		\label{fig:orientation_computation}
	\end{figure}
	
	Because of depth sensor inaccuracy, the computed map pixel coordinate doesn't always lie on the wall. In figure \ref{fig:orientation_computation}, computed face pixel is coloured red. \\
	
	Using a simple breadth first search in face pixel position proximity, the closest wall pixel is determined. In figure \ref{fig:orientation_computation}, this pixel is also coloured red. \\
	
	After wall pixel has been found, the Hough line finding algorithm is applied to an area around wall pixel. The detected lines in figure \ref{fig:orientation_computation} are represented with green colour. If more than one line is found, the line which is closest to the wall pixel is selected. After the line is found, there are two possible orientations as represented in figure \ref{fig:orientation_computation}. The vector that is pointing in the direction of the robot is chosen to be the face orientation. \\
	
	A similar algorithm is used to detect ring orientation.
	
	\subsubsection{Face classification}
	After faces are detected and localized in the world, the faces are identified.
	
	\subsection{Colour classification}
	The robot must be able to detect ring and cylinder colours. There are six possible classes: black, white, red, green, blue and yellow. \\
	
	In homework 2, we tested different colour spaces and classification models and concluded that the k-nearest neighbours algorithm worked best. The colour space with the highest accuracy in homework was \texttt{HSV} colour model, but in the Gazebo simulator, \texttt{RGB} space worked better. \\
	
	So the colour classifier in the final task uses the k-nearest neighbours algorithm and takes in an input vector in $(red, green, blue)$ format and returns a colour label. \\
	
	Because of the non-even lighting in the Gazebo simulator (and probably real world too), the classifier sometimes returns incorrect label. The problem was solved using the robustification process as described in the \ref{robustification} section. Colour classifier is run multiple times on different object detections and the most frequent colour is chosen as the object colour. \\
	
	\subsection{Rings}
	In this section, the algorithm for ring detection is presented.
	
	\subsubsection{Ring detection}	
	\subsubsection{Ring color detection}
	
	\subsection{Cylinder detection}
	\subsubsection{Removal of planes}
	\subsubsection{Cylinder detection}
	\subsubsection{Approaching point computation}
	
	\subsection{Robustification} \label{robustification}
	The sensor data that the robot receives is not always accurate. When object is detected, the depth sensor might have computed the distance inaccurately or the robot was not localized. The detectors might also return false positives and so on. \\
	
	To combat this, a robustification is performed on detected objects. The object detectors are sending detections to robustifier, which collects data and determines if the detection is a true positive or not. \\
	
	For detection to be considered true positive, the following must hold:
	\begin{enumerate}
		\item Detected object should be detected at least \texttt{threshold} times
		\item There must be at least \texttt{minimum distance} meters between this detection and every other previous detection.
	\end{enumerate}

	If two detections are close enough (the distance is less than \texttt{minimum distance}), they are considered the same object. The world position and approaching point position of those two detections are averaged. So the object position is closer to its actual position in space.
	
	\subsection{Smart exploration of space}
	\pagebreak
	
	\subsection{Fine movement}
	The \texttt{move\_base} package can be used with \texttt{ROS} to move the mobile base around the map. It uses local and global planner information to avoid obstacles and try to reach the given goal. If it fails to reach the goal, it tries to clear its local map. If that fails, the robot stops trying to reach the goal and notifies us that it has failed to move to the goal. \\
	
	The most common reason for failure is that the goal is too close to any wall in our world. In our final task, the robot had to be able to pick up rings that are positioned very close to the wall. So close actually, that the package is unable to reach the goal. To do that, odometry data must be used combined with \texttt{Twist} messages to move the robot manually. \\
	
	Because the robot must be able to visit the ring from specific direction (ring orientation), we used a modified version of \texttt{A*} algorithm, that not only finds the shortest path between two points, but also uses the starting and ending robot orientation. \\
	
	Because the world is flat and the robot can only rotate along one axis, any robot pose can be represented using a tuple $(x, y, \varphi)$, where $\varphi$ is the orientation along the vertical axis. In theory, the $\varphi$ can be any real number, but in our case, the robot will only have a finite number of possible orientations (the first one being $[0, \frac{2\pi}{n}]$, where $n$ is the number of all orientations). In our final task, we found that $n = 8$ or $n = 16$ works best. \\
	The algorithm will use map to access environment information, so the $x$ and $y$ represent pixel coordinate in map coordinate system.
	
	% Image of the robot and the next three possible states
	\begin{figure}[h]
		\centering
		\vspace{6cm}
		% \includegraphics[width=4cm]{images/orientation_computation}
		\caption{Next possible robot states}
		\label{fig:next_robot_states}
	\end{figure}
	
	If the robot is currently in state $\vec{s_i} = (x_i, y_i, \varphi_i)$, we can define the next states to be:
	
	\begin{enumerate}
		\item Move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i)$$
		\item Rotate left by $\frac{2\pi}{n}$ and move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i - \frac{2\pi}{n})$$
		\item Rotate right by $\frac{2\pi}{n}$ and move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i + \frac{2\pi}{n})$$
	\end{enumerate}
	
	Where $\alpha$ is the step size (to move 1 pixel forward, $\alpha = 1.5$). Each state is only possible if the map does not contain a wall at pixel $(x_{i+1}, y_{i+1})$. Because the orientations are cyclical, if the robot orientation is less than $0$ or more than $2\pi$, the orientation is wrapped around to be inside $[0, 2\pi]$ range. \\
	
	The heuristic function can be defined as an Euclidean distance between current state and the end state. $$h(s_i) = \sqrt{(x_{end} - x_i)^2 + (y_{end} - y_i)^2 + (\varphi_{end} - \varphi_i)^2}$$. To avoid getting too close to the walls, a penalty is added to the pixels that are too close to the wall. \\
	
	The \texttt{A*} algorithm can now be run using states as defined above. We start with initial robot state $(x_0, y_0, \varphi_0)$ and set the goal to the ring position $(x_m, y_m, \varphi_m)$. The closest path between those two states is the path that the robot should follow to get from its current position to the goal position including its orientations. \\
	
	After the path is computed, the robot can subscribe to odometry data to get its position and follow the list of poses that the previous algorithm has computed. To move from one pose to another, we wait until next odometry message is received, then rotate until we are oriented in the direction to the next goal and then move until we are close enough to the next pose. \\
	
	\section{Implementation and integration}

	% Image of our final arhitecture
	
	\subsection{Object detectors}
	There are three different nodes responsible for object detections. Face and ring detectors are using \texttt{rgb} and depth images and the cylinder detector is using point cloud and \texttt{rgb} image to detect object in our world. \\
	
	All three object detectors are publishing a custom \texttt{ObjectDetection} message type. The \texttt{Robustifier} component subscribes to this type of message and tries to minimize the number of false positives. 
	
	\subsubsection{ObjectDetection message}
	
	\subsubsection{Face detector node}
	\subsubsection{Ring detector node}
	\subsubsection{Cylinder detector node}

	\subsection{Robustifier}
	
	\subsection{Brain}
	
	\subsection{Movement controller}
	
	\subsection{Greeter}
	
	\section{Results}
	\section{Division of work}
	% We should only write what each of us did here and add that the professor should compute percentages himself.
	
	\section{Conclusion}	
	
\end{document}