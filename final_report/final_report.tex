\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{arrows,automata}
\usepackage[bottom]{footmisc}

\author{Team Gamma \\ {\small Ajda Frankovič, Martin Preradović, Niki Bizjak}}
\title{Final report}
\date{}
\begin{document}
	
	\maketitle

	\tableofcontents
	
	\section{Introduction}
	In this report we will explain how our robot by the name of Erazem performs the task of a skilled match maker. He was hired by Gargamel, who wanted to find love and get married as quick as possible. Erazem first had to find Gargamel to ask him about his preferences. He already knew that Gargamel was looking for a woman, but since usualy his clients also cared about the looks, he also wanted to find out about this kind of preferences. \\
	
	When Gargamel suprised him with his simple answer, that he only cared about the hair colour and hairstyle, he couldn't help but think to himself, what a strange man Gargamel was. He shook of this thought since he had had much stranger clients before and started looking right away. \\

	He was scanning the place for the women. For each face he saw, he quickly filled his mental checklist with Gargamel preferences. "Such easy criteria", he thought to himself, "but so little matches..." When he finally came across a woman that matched Gargamel's preferences, he fixed his tie, took a deep breath, and approached her. She seemed interested in meeting Gargamel but was very secretive. When Erasem asked her to tell him something about herself, she only told him her favourite colour. Erazem firmly remembered this little detail and after some chit-chat said goodbye to her. He was happy to have finally found a potential suitor but at the same time a little worried about the lack of information he had about her. \\
	
	Erazem thought it is time to report to Gargamel about his findings. They met at Gargamel's and Erasem told him all he knew about that woman. It was up to Gargamel to decide wheteher she seemed promising or not. If the Gargamel was to decline this suitor, Erazem would have gone out again and found him another one. When Gargamel liked the suitor Erazem suggested, he told him a bit nervously that he believed that tossing a coin into the wishing well and saying a wish makes that wish come true. Excellent matchmaker as Erazem was, he immediately understood. He agreed to find a wishing well in the woman's favourite color and toss a coin in it while also saying the wish for Gargamel and the woman to marry and live happily ever after. \\

	Erazem searched through his memory whether he has already seen the well in this colour. If he could remeber, he would have definitely known where to go. After some time, he found the right wishing well, tossed the coin and said the wish. Now he went about to find a perfect ring. Of course, it had to be in the woman's favourite colour since Gargamel didn't have musch opinion on which colours he liked. He picked it up and brought it to the woman. \\
	
	This was the moment of truth. Will she agree to mary Gargamel, having received such thoughtfull gift? If she agrees, Erazem's work here is done. But what if she isn't impressed? Well, then Erazem will find another potential suitor and charge Gargamel some extra money, for the inconvenience. \\

	He knew that his job was very impactfull. He promised himself not to let Gargamel down. And the reward at the end will be beautiful. Two lost souls finding each other and living their happy ever after. \\

	In the following sections, we explain the magic behind Erazem's matchmaking skills.
	
	\section{Methods} \label{methods}
	Here we present the algorithms and the methods that were used in our system.

	% Dodaj nek dictionary pojmov
	% To lahk anardimo v vsaki sekciji, kjer je smiselno, al pa sam na začetku. Sam po mojem si je lažje zapomnat in je manj overwhelming, če je mal pa po mal, torej v vsaki sekciji sproti, kar se rabi.
	% Izraze, ki jih definiramo tekom pisanja, lahka pišemo znotri \texttt{}, torej "...orientation of the face (from now on, \texttt{face orientation})..."
	% kaj je camera frame
	% kaj je camera coordinate system
	% kaj je world coordinate system
	% kaj je world frame
	% to se mogoče še najlepše razloži kr s slikico
	% world coordinates
	% map coordinates
	
	\subsection{Camera pixel to world position} \label{pixel_to_world}
	Object detectors must be able to compute the positions of the detected objects in the world. The robot is equipped with Kinect sensor that provides colour and depth images of its surroundings. The object detection is performed on received images. When the detector detects an object it computes its position in the image plane (that is, pixel $(u, v)$ in the image where the object appears). Using pixel $(u, v)$, depth image and some additional camera information, the object's world position can be computed. \\
	
	But before the computation can be performed, both colour and depth images must be synchronized. Because the robot is moving and the environment is changing very fast, even a small delay between images can cause computed world positions to be inaccurate. This problem can be seen in figure \ref{fig:non_synchronized_raw_detection}. \\ 
	
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm]{images/detections}
		\caption{Inaccurate detection of the object's position due to lack of time synchronization among depth and colour image}
		\label{fig:non_synchronized_raw_detection}
	\end{figure}

	Using camera calibration matrix, focal length and image format can be obtained. Focal length is the distance from the centre of camera coordinate system to image plane. The image format gives us information about the width and height of the camera sensor. The camera also has additional information about its position in the world (namely, its position and rotation). \\

	\begin{figure}[h]
		\centering
		\caption{Pinhole camera model}
		\label{fig:pinhole_camera_model}
	\end{figure}
	
	% Damo neko podobno sliko kot na naslednji povezavi
	% https://alicevision.readthedocs.io/en/latest/_images/pinholeCamera.png
	
	Assuming that the camera is using pinhole camera model (and the camera calibration matrix is known), a ray can be "cast" from the centre of the camera coordinate system to the pixel on image plane, where the object was detected. This can be seen in the figure \ref{fig:pinhole_camera_model}. Using depth image information, we can obtain the distance from the centre of the camera to the detected object in space. Using the computed ray and the distance to the object, we can compute object's position in a three-dimensional space in camera frame. \\
	
	After that, we can use a transformation matrix to finally convert the position of the detected object from the camera frame to the world coordinate system. \\ 
	
	In our system, the face and ring positions are computed this way, meanwhile the detection of cylinders works with point clouds, where the points are already computed in the world frame. \\
	
	The algorithm explained in this section is used when localizing detected faces and rings. Cylinders are localized using \texttt{PointCloud} information (which is in fact computed from the depth image with a similar approach). \\
	
	\subsection{Faces}
	In this section, we present algorithms that our robot uses to detect faces, compute their orientation in space and classify them.
	
	\subsubsection{Face detection} \label{face_detection_algorithm}
	
	Face detection was done using Haar cascade face detection algorithm. We chose this algorithm because it can be run in real-time and it has a high detection rate. \\
	
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
			\fill[lightgray] (0,0.5) rectangle (4,4.5);
			\fill[black] (1,2.75) rectangle (3,3.5);
			\fill[white] (1,2) rectangle (3,2.75);
		\end{tikzpicture}
		\caption{An example of a good haar feature}
		\label{fig:haar_features}
	\end{figure}

	An example haar feature is displyed in figure \ref{fig:haar_features}. The grey region represents the face region (which fits well in a square). The black and white region is a haar feature. This specific feature performs well in human eye region detection. It detects the transition from eyes to forehead. \\
	
	The face images are first cropped to the same size and aligned so that the eyes approximately match. Then, a set of Haar features is generated. Each feature is defined at certain position in the face rectangle and consists of black and white regions (see figure \ref{fig:haar_features}). The value of the feature is computed as as difference between the sum of pixel intensities in the dark regions and the sum of pixel intensities in the light regions. \\
	
	Then, for each feature, a simple and fast binary classifier is trained on the training data that can recognize if it is looking at a face or not. Not all features prove to be very good at this task, so the features are ranked by their classification accuracy. With a boosting machine learning algorithm, many weak classifiers (as described above) are used to improve face detection accuracy. A cascade of weak detectors is created such that we start with the most accurate classifier and continue with less accurate ones. This gives the haar cascade algorithm its real-time ability. For some regions, the first few classifiers detect that there is definitely not a face and the detection can stop. This way, the most processing power is given to the areas that most likely contain a face. \\
	
	The detection is then done with a sliding window method. This simply means that we are evaluating every possible rectangle area in the image. If we want to detect faces of different scales, the image must be resized and the entire algorithm is repeated. So the cascade is crucial for speed here. \\
	
	\subsubsection{Computing the orientation of the faces} \label{face_orientation_computation}
	After the face has been detected in an image, it's position in the world coordinate system is computed. The approaching point is then calculated using static map information. \textbf{Approaching point} is a point close to the face and directly in front it that the robot must visit to approach the face. \\
	
	To compute the approaching point, the orientation of the detected face must first be determined. The face orientation is the direction in which the face is pointing at. In order to do that, we need the static map information. \\
	
	Static map is the map of the robot environment. It is used for robot localization and path computation. In our case it was generated before the task, though algorithms that can explore unknown space exist and could be used to enable our robot to perform this task in unseen environment. In ROS, the map is simply an image with some additional metadata. Each pixel on map represents a small part of the world (in our case, each map pixel represents 5 centimetres in the world).\\
	
	To compute face orientation, its position in the world coordinates is first converted to map coordinates. Then, the corresponding pixel in map is calculated. \\
	
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
			\draw[step=0.5cm,lightgray,very thin] (0.1,1.1) grid (6.4,4.9);
			\fill[darkgray] (0,3.5) rectangle (6.5,4);
			\fill[red] (3,3.5) rectangle (3.5,4);
			\fill[red] (3.25,1.75) circle (0.15);
			\node[] at (4,2) {$(x, y)$};
			\draw [->] (1.75,3.75) to (1.75,2.75) node [right] {$\vec{n}$};
			\draw [->] (1.75,3.75) to (1.75,4.75) node [right] {$-\vec{n}$};
			\draw[very thick, black] (-0.5,3.75) -- (7,3.75);
		\end{tikzpicture}
		\caption{Face orientation computation}
		\label{fig:orientation_computation}
	\end{figure}
	
	Because of depth sensor inaccuracy, the computed map pixel coordinate doesn't always lie on the wall. In figure \ref{fig:orientation_computation}, the computed face pixel is marked with a red circle. \\ 
	
	Using a simple breadth first search in the proximity of the pixel which corresponds to the position of the detected face (from now on face pixel), the closest pixel from the set of pixels, which correspond to the positions occupied by the walls (from now on wall pixel), is determined. In figure \ref{fig:orientation_computation}, this pixel is coloured red. \\
	
	After the wall pixel has been found, the Hough line finding algorithm is executed on an area around it. The detected line in figure \ref{fig:orientation_computation} is represented in black. If the algorithm finds more than one line, the line which is closest to the wall pixel is selected. \\
	
	Because the face is positioned on the wall, on of the wall normals can be chosen as the face orientation. But there are two possible options (as represented in figure \ref{fig:orientation_computation} with labels $\vec{n}$ and $-\vec{n}$), pointing in the opposite directions. The normal that is pointing in the direction of the robot is chosen because otherwise the robot would not be able see the face. \\
	
	Similar algorithm is used for computation of the ring orientation, but instead of the normal of the wall, the direction of the wall is used as the ring orientation. \\
	
	\subsubsection{Face classification}
	After faces are detected and localized in the world, they have to be identified. For this task we used a pretrained convolutional neural network to create embeddings and then we trained our own model to recognize faces. \\

	The first step in this method is obtaining encodings of our faces by mapping them into an embedding. An embedding is a low-dimensional space into which we map high-dimensional vectors, in this case images. An image gets translated into the embedding by a trained neural network, creating a 128 dimensional vector representation. This is an encoding of the image. In this way we capture some of the semantics of the faces, as similar faces get placed closer together than different faces. \\

	The neural network that we used for our task is a convolutional neural network. It differentiates itself from a ordinary neural network by having convolutional deep layers, which can apply filters to the images, that detect patterns, such as edges and corners and in later layers maybe even specific objects such as eyes or ears. Training of this network is done using triplets. In a single step of the training process, we feed the neural network three images, and thus generate three 128 dimensional encodings. Two images represent faces of the same person, while the third image is a random face, that represents a different person as the one in other two images. Then we tweak the weights in such a way, that encodings of the two faces of the same person get mapped closer together, whereas the third face gets placed farther away. \\

	We could use only one encoding of a certain face, compare it to new faces and still get very good classification results, as was shown in our report for homework 3 with the k-nearest neighbours model for k is 1. However we wanted to make our face recognition more accurate and robust. That is why we trained a separate support vector machine model on the encodings we obtained from the neural network. \\

	% TODO: finish face classification
	% should we explain how we gathered training data?
	
	\subsection{Colour classification} \label{colour_classification}
	The robot must be able to detect the colour of the rings and cylinders. There are six possible classes: black, white, red, green, blue and yellow. \\
	
	In homework 2, we tested different colour spaces and classification models and concluded that the k-nearest neighbours algorithm worked best. The colour space with the highest accuracy in homework was \texttt{HSV} colour model, but in the Gazebo simulator, \texttt{RGB} space worked better. \\
	
	So the colour classifier in the final task uses the k-nearest neighbours algorithm and takes in an input vector in $(red, green, blue)$ format and returns a colour label with one of the six possible classes listed above. \\
	
	Because of the uneven lighting in the Gazebo simulator (and probably in the real world too), the classifier sometimes returns an incorrect label. We solved this problem by using the robustification process as described in the \ref{robustification} section. Colour classifier is run multiple times on different detections of the same object and the most frequent colour is chosen as the colour of the object (from now on object colour). \\
	
	\subsection{Rings}
	In this section, the algorithm for ring detection and its approaching point computation is presented.
	
	\subsubsection{Ring detection} \label{ring_detection}
	When robot finds the woman that is willing to marry Gargamel, it must help him find a ring that she will like. Luckily, she is kind enough to tell us her favourite colour. The robot must then find the ring in that colour, give it to her and ask her to marry him. \\
	
	But before the ring can be picked up, it must first be detected by the robot. The ring detection is performed on the depth image and the ring colour detection is performed on the \texttt{rgb} image. Again, both images must be synchronized, so it can be accurately localized. \\
	
	% I am not sure that the information below is correct
	Depth image is computed from disparity image, which is calculated from Kinect stereo camera system. The further away an object is, the smaller its disparity will be. Using calibration matrix, depth of each pixel can be approximated from disparity image. Object that appear further away from the camera have a smaller disparity, which reduces the accuracy of depth computation. To combat this, our algorithm first removes all objects that are over a certain distance away from the camera. \\
	
	After inaccurate depths are removed, a blob detection algorithm is applied to our depth image. The algorithm finds regions in our image that differ in colour. It actually searches for dark areas (areas that are further away from the camera) in the depth image. Because the inside of the ring is darker than the ring itself, the inner part is considered a blob. But not all blobs are considered rings. We can then apply some domain knowledge to the problem. We know that all the rings are positioned 11 cm above the cubes, so we can filter all blobs that lie below that height. If we look at the ring from any angle, they look elliptical, so the blobs are filtered by their roundness too. \\
	
	\begin{figure}[h]
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.8\linewidth]{images/ring_detection}
			\caption{Ring detection}
			\label{fig:detected_ring}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.8\linewidth]{images/ring_detection_cropped}
			\caption{Cropped ring detection}
			\label{fig:detected_ring_cropped}
		\end{subfigure}
		\caption{Ring detection using blob detector}
		\label{fig:ring_detection}
	\end{figure}

	In figure \ref{fig:detected_ring}, we can see an example of detected ring. Even though the corner of the cube is overlapping with the circle, the ring is still detected. This also presents another problem. To localize the ring, we must know the distance to the ring, but which distance should we use? \\
	
	To compute the distance from camera to the ring, a histogram of distances is created. In figure \ref{fig:distance_histogram}, we can see an example histogram for the figure \ref{fig:detected_ring_cropped} with 24 bins. As we can see, there are two bars that stand out. One of them is the edge of the cube and the other is the ring. A mask is constructed so that only distances that lie in the highest bucket are retained, everything else is removed. \\
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
			ybar,
			xlabel={Distance}, ylabel={Number of pixels},
			xmin=0, xmax=23, ymin=0, ymax=1400,
			xtick={0,4,8,12,16,20,24},
			ytick={0,200,400,600,800,1000,1200,1400,1600},
			width=12cm, height=5cm
			]
			\addplot[fill=gray]
			coordinates {
				(0, 0) (1, 0) (2, 0) (3, 0) (4, 0) (5, 0) (6, 0) (7, 0) (8, 306) (9, 0) (10, 1261) (11, 0) (12, 0) (13, 0) (14, 0) (15, 0) (16, 0) (17, 0) (18, 0) (19, 0) (20, 0) (21, 0) (22, 0) (23, 0) 
			};
			\end{axis}
		\end{tikzpicture}
		\caption{Distance histogram}
		\label{fig:distance_histogram}
	\end{figure}

	The mask is then applied to both, the distance image and the colour image. In figure \ref{fig:masked_colour_ring}, we can see the result of filtering of the \texttt{rgb} colour image using the mask obtained in the previous step. \\

	The distance to the ring is computed as the average of distances to each pixel in the masked distance image. The ring position in world frame is computed using as explained in section \ref{pixel_to_world}. \\

	\subsubsection{Ring approaching point computation}
	Computation of approaching points for rings takes advantage of the static map, which provides information about the location of the walls, the unknown space (outside of the map and closed areas inside the map, such as boxes) and the free space, the space through which the robot can move. Using this information and the location of the ring detection we can find the closest wall and move approaching point away from it, in the direction of the normal. \\

	First we have to find the wall closest to our detection. We can do this by searching for the closest straight line (edge) using \texttt{Hough Transform} algorithm. \texttt{Hough Transform} takes an edge image as the input. \textbf{Edge image} is usualy a binary image where one of the binary values 0 and 1 marks the pixels that contain the edge and the other value marks everything else. This kind of image can be obtained by various edge detectors that detect edges by looking for drastic changes in intensity in the original image. \\
	
	Each line from the edge image can then be represented with an equation of the form $y = ax + b$ and mapped to a point $(a, b)$ in the \texttt{Hough Space}. Because this system cannot represent the vertical lines since $a = \infty$, the line is rather represented as $\rho = x\cos(\theta) + y\sin(\theta)$ and mapped to a point $(\theta, \rho)$. Each point from the line therefore produces a cosine curve in \texttt{Hough Space}. If two points lay on the same line, their cosine curves in the \texttt{Hough Space} intersect in one $(\theta, \rho)$ point. The \texttt{Hough Transform} algorithm detects lines by searching for the $(\theta, \rho)$ pairs that record more than some $threshold$ number of intersections. \\

	For each detection we search the area around it for the lines and then find the one that is the closest to the detection. This line is represented with two points, both ends of the line. If we subtract one point from another, we get a vector parallel to the wall. By rotating it by 90 degrees we get a normal $n$ that is perpendicular to the wall. We use static map information to determine whether the neighbouring pixel of the line midpoint in the direction of the $n$ lies in the space accesible to the robot or not. If yes, we take $n$ as our direction vector for setting the approaching point. If not, we take $-n$. \\

	Figure \ref{fig:ring_approaching_point_computation} shows the free space (white) and the inaccessible space (light gray) separated by the wall and the two normals to the wall. Depending on wheteher the location of the ring is detected correctly (black dot) or incorrectly (red dot) our normal may be either one of the two shown in the Figure \ref{fig:ring_approaching_point_computation}. For setting the approaching point we want a normal to point towards the free space as does $n$ in the figure below.

	\definecolor{verylightgray}{gray}{0.90}
	\begin{figure}[h]
		\centering
		\begin{tikzpicture}
			%non-free space
			\fill[verylightgray] (0,4) rectangle (5.5,5.4);

			% grid
			\draw[step=0.5cm,lightgray,very thin] (0.1,2.1) grid (5.4,5.4);

			%wall
			\fill[darkgray] (0,3.5) rectangle (5.5,4);

			% correct ring detection
			\fill[black] (3.25,2.75) circle (0.15);
			%\node[] at (3.5,2.5) {$(x, y)$};

			% incorrect ring detection
			\fill[red] (3.25,4.75) circle (0.15);

			\draw [->] (1.75,3.75) to (1.75,2.75) node [right] {$\vec{n}$};
			\draw [->] (1.75,3.75) to (1.75,4.75) node [right] {$-\vec{n}$};
			\draw[very thick, black] (-0.5,3.75) -- (6,3.75);
		\end{tikzpicture}
		\caption{Computing approachin point for ring}
		\label{fig:ring_approaching_point_computation}
	\end{figure}

	Now we move along the calculated direction vector for 0.45 meters and set this position as the position of the approaching point. Orientation for the approaching point should point towards the line so we just invert the direction vector by multiplying it by $-1$ and set this as the orientation of our approaching point.

	\subsubsection{Ring colour detection}
	The ring colour is computed using the mask that we have calculated in section \ref{ring_detection}. The colour  image is filtered so that only ring is left and then the colour is averaged. The average colour is then classified using the algorithm explained in section \ref{colour_classification}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[height=5cm]{images/ring_detection_colour}
		\caption{Masked colour ring}
		\label{fig:masked_colour_ring}
	\end{figure}
		
	\subsection{Cylinder detection}
	In this section the algorithm for cylinder detection from point clouds using RANSAC is presented. In short, we first had to remove all larger planes from the point cloud and fit the remaining points to a cylinder model. But first we had to do some preprocessing on the input point cloud data before our algorithm could be run. Namely, making the point cloud sparser in order to reduce the execution time of the algorithm and filtering out points beyond some distance from the robot in order to avoid errors in model fitting, because of the point cloud being too sparse. \\

	The backbone of our cylinder detection algorithm is fitting a point cloud to a certain model using RANdom SAmple Consesus (RANSAC). This is an iterative method that is used to estimate parameters of a mathematical model from a set of data containing outliers. In every iteration RANSAC selects a random subsample of the input data. These points are then considered inliers and the hypotesis is tested in a few steps. First the model is fitted to the inliers, then all data points are tested against the fitted model and, if they fit well they are also considered inliers. After that the model is reestimated from the original and new inliers and finally, it is evaluated by estimating the error of the inliers relative to the model. After a fixed number of iterations, the best model is chosen. \\
	
	\subsubsection{Removal of planes}
	To be able to detect cylinders efficiently and robustly, we need first remove all planes from the point cloud in order to reduce the computational load and avoid many flat surfaces and corners from being detected as cylinders. We do this by iteratively fitting the point cloud to a plane model with RANSAC, as explained above, and then removing all the inliers from the point cloud. This way we remove all large planes in the point cloud that have more inliers than some empirically set threshold. \\
	
	A threshold is used so that the points that are part of the cylinders are retained, because a small enough area of the cylinder might be considered a plane, and thus be removed from the point cloud. This parameter had to be fine tuned in order to remove as many planes from the point cloud, but also keep the cylinder inliers. \\

	\subsubsection{Cylinder detection}
	After all or most of the planes have been removed from the point cloud, we can detect cylinders. The method for this is very similar to the one used for detecting planes, but instead of fitting a point cloud to a plane, we fit it to a cylinder. This is trickier, because cylinders are more complex objects with more paramateres and thus, it is harder to fit the points to the model. The parameters for this method had to be carefully chosen in order to accurately detect cylinders and avoid incorrect detections such as corners. \\

	We chose to only detect one cylinder in a point cloud, first, because usually they are relatively far apart, so there is no fear that we could miss one and second, because the farther the object is from the robot, the sparser the point cloud and consequently more errors are present, such as mistaking a cylinder for a plane or vice versa. \\
	
	\subsubsection{Cylinder approaching point computation}
	The cylinder approaching point computation is not as difficult as the one for faces. If cylinder has been detected from current robot position, that means that the robot must have a clear view of the cylinder. Because of that, the approaching point can be computed as a point on the line connecting the robot and the cylinder. \\
	
	A point that lies 0.6 metres from the centroid of the cylinder on this line is selected as the approaching point. If the point can't be reached (that is, if it is too close to the wall), approaching point is repositioned so that there is enough space for the robot to visit the point. This is done by finding the closest wall on the map and then moving the point in the opposite direction. This is repeated until there is no wall too close to the point. \\
	
	\subsection{Robustification} \label{robustification}
	The sensors are not 100\% accurate, so we have to take into account some deviation. For example, when an object is detected, the depth sensor might compute the distance inaccurately, or the robot may not have been localized. Detectors might return false positives and so on. \\
	
	To combat this problem, a robustification is performed on the detected objects. The object detectors send detections to robustifier, which collects data and determines if the detection is a true positive or not. \\
	
	For detection to be considered a true positive, the following must hold true:
	\begin{enumerate}
		\item Detected object was detected at least \texttt{threshold} times
		\item There is at least a \texttt{minimum distance} meters distance between this detection and every other previous detection.
	\end{enumerate}

	If two detections are close enough (the distance between them is less than \texttt{minimum distance}), they are considered the same object. The positions of those two detections in the world are then averaged, so the object position is closer to its actual position in space.
	
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm]{images/detections}
		\caption{Many inaccurate object detections}
		\label{fig:inaccurate_detections}
	\end{figure}

	Figure \ref{fig:inaccurate_detections} shows why the robustification process is needed. Face detector works very fast, but the localization isn't that accurate. Each red sphere represents one face detection. By averaging many face detections, the pink sphere position is obtained. As we can see, the the averaged position is much closer to the real face position (which is in this case on the wall near this averaged position) than each individual detection. The distance between the farthest points in figure \ref{fig:inaccurate_detections} is 0.3 metres, which is another reason why we think robustification process is needed. \\
	
	It should also be noted that similar process is applied to the approaching points. Approaching point is computed for each face detection and as positions of the face detections are averaged, approaching points are averaged as well. In figure \ref{fig:inaccurate_detections}, this point is yellow. \\
		
	\subsection{Smart exploration of space} \label{smart_exploration}
	There are two ways to determine some goals for space exploration; either you hardcode a list of goals or you create a system that automatically sets the goals based on the shape of the map. Hardcoding is almost never a good idea, se we went with the latter. \\

	Our algorithm uses static map as the input. First, the contrast between the walls or unknown space, and the space where robot can move, is amplified. After that a morphological operation of closing is performed to smooth out the corners which robot can't reach because of it's circular shape. \\
	
	The part containing the relevant map is cropped out of the whole image as the other empty parts are not needed in the rest of the process. We then find the corners with \texttt{Harris corner detector} and then apply the \texttt{non maxima suppresion} to only keep the actual corners. The corners are corners in the walls and obstacles and also the ends of the walls. \\
	
	An edge is a sudden change in brightness and a corner is the point where two edges meet. Corners are the points, where gradient change is big in all directions. Harris corner detector works on a grayscale image, smoothed by a \texttt{Gaussian filter} (removes any noise). For each pixel in the image, it considers a 3x3 window around it and uses derivatives to computes a \texttt{Harris value} for it. \texttt{Harris value} is a score which tells how likely does the pixel contain a corner. \texttt{Non maxima suppression} is used to only declare corners in the pixels, whose score is the local maximum of the certain window and also exceed some threshold value. \\

	With the list of corners in the map ready, we can perform \texttt{Delaunay triangulation} to split the space without obstacles into triangles. \texttt{Delaunay triangulation} is a special kind of triangulation, where  no point form the set that the triangulation is performed on is inside the circumcircle of any triangle in the trinagulation. Another property of the \texttt{Delaunay triangulation} is that it maximizes the minimum angle in the triangles and thus create more even distribution of the area. \\

	For each triangle, our algorithm find its centroid - the center of the circumcircle. If we were to connect the centroids, we would obtain a so called Voronoi diagram, a partition of the plane into regions. Each region contains all of the points that are closer to the seed inside this region, in our case a corner, than to any other seed (corner). The points from the \texttt{Delaunay triangulation} (centroids) are therefore as far away from all walls as possible. This makes them great points for space exploration as they are far from the obstacles and somewhat evenly distributed across the map. \\

	We have to be careful as centroids form the \texttt{Delaunay triangulation} will also be set in narrow passageways to which robot may not have access. And sometimes, two points might end up very close together, which isn't ideal for the exploration. \\
	
	To eliminate the second problem, we use \texttt{hierarhical clustering} with the minimum distance as the stopping criteria. This merges any points that are too close together into one, their average. It does so hierarhically, in each iteration it merges only the two points that are the closest. \\
	
	This step can sometimes also solve the first problem. To be sure that the robot can reach all of the exploration goals, we check if any goal is too close to any wall and if so, move it into the opposite direction, away from the wall, if possible. \\

	In the figure below, we can see the \texttt{Delaunay triangulation} of our map in green and the centroids in red. The points in blue represent the exploration goals after \texttt{hierarhical clustering} and moving points away from the walls.

	\begin{figure}[h]
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[height=5cm]{images/before}
			\caption{Before processing}
			\label{fig:before_clustering}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[height=5cm]{images/after}
			\caption{After processing}
			\label{fig:after_clustering}
		\end{subfigure}
		\caption{\texttt{Delaunay triangulation} and exploration goals before (a) and after the \texttt{hierarhical clustering} and moving unreachable ones to reachable position (b).}
		\label{fig:exploration_goals}
	\end{figure}
	
	\subsection{Fine movement} \label{fine_movement}
	Fine movement is a special way in which our robot can move. It is used when the default movement wouldn't be able to get us to the goal even though the goal is reachable.  \\

	The \texttt{move\_base} package can be used with \texttt{ROS} to move the mobile base around the map. It uses local and global planner information to avoid obstacles and try to reach the given goal. If it fails to reach the goal, it tries to clear its local map. If that fails, the robot stops trying to reach the goal and notifies us that it has failed to move to the goal. \\
	
	The most common reason for failure is that the goal is too close to any wall in our world. The robot builds a costmap of its environment, where the closer the position is to the wall, the highest cost it has. This way a robot can avoid obstacles and plan the path from point A to point B. The robot builds a costmap in a way that it makes sure not to bump into anything, which means it also takes into account all possible errors in data from its detectors. This means that some of the points that the robot can reach in reality unfortunately fall into the high-cost area and so the normal movement cannot get us there because the risk of bumping into something is too high. \\

	In our final task, the robot had to be able to pick up rings that are positioned very close to the wall and get very close to the cylinders so that it could reacha bove it and toss a coin. Both required positions are so close to the obstacle/wall, that the default movement component is unable to reach the goal. To do that, odometry data must be used and combined with \texttt{Twist} messages to move the robot manually. \\
	
	\begin{figure}[h]
		\begin{subfigure}{.5\textwidth}
			\centering
			\begin{tikzpicture}
			\draw[lightgray, very thin] (-0.5, -0.5) rectangle (4.5, 4.5);
			% Robot
			\draw[thick] (1.5, 1) circle (0.4);
			\draw [->, thick] (1.85951761852, 1.17534845872) to (2.3987940463, 1.43837114679) node [right] {};
			\fill[black] (1.5, 1) circle (0.025);
			% Ring
			\fill[black] (2, 3) circle (0.1);
			\draw [->] (2, 3) to (2 - 0.3, 3) node [right] {};
			\end{tikzpicture}
			\caption{Step 1}
			%\label{fig:detected_ring}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\begin{tikzpicture}
			\draw[lightgray, very thin] (-0.5, -0.5) rectangle (4.5, 4.5);
			% Robot
			\draw[thick] (1.5, 1) circle (0.4);
			\draw [->, thick] (1.59701425001, 1.38805700006) to (1.74253562504, 1.97014250015) node [right] {};
			\fill[black] (1.5, 1) circle (0.025);
			% Ring
			\fill[black] (2, 3) circle (0.1);
			\draw [->] (2, 3) to (2 - 0.3, 3) node [right] {};
			\end{tikzpicture}
			\caption{Step 2}
			%\label{fig:detected_ring_cropped}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\begin{tikzpicture}
			\draw[lightgray, very thin] (-0.5, -0.5) rectangle (4.5, 4.5);
			% Robot
			\draw[thick] (2, 3) circle (0.4);
			\draw [->, thick] (0.5 + 1.59701425001, 2 + 1.38805700006) to (0.5 + 1.74253562504, 2 + 1.97014250015) node [right] {};
			% Ring
			\fill[black] (2, 3) circle (0.1);
			\draw [->] (2, 3) to (2 - 0.3, 3) node [right] {};
			\end{tikzpicture}
			\caption{Step 3}
			%\label{fig:detected_ring}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\begin{tikzpicture}
			\draw[lightgray, very thin] (-0.5, -0.5) rectangle (4.5, 4.5);
			% Robot
			\draw[thick] (2, 3) circle (0.4);
			\draw [->, thick] (2 - 0.4, 3) to (0.5 + 2 - 1 - 0.4, 3) node [right] {};
			% Ring
			\fill[black] (2, 3) circle (0.1);
			\draw [->] (2, 3) to (2 - 0.3, 3) node [right] {};
			\end{tikzpicture}
			\caption{Step 4}
			%\label{fig:detected_ring_cropped}
		\end{subfigure}
		\caption{Fine movement}
		\label{fig:fine_movement}
	\end{figure}

	% chapter about setting the additional approaching points before using fine movement
	% RINGS:
	% Orientation for the approaching point is obtained by rotating the direction vector 90 degrees to the right. This sets the orientation vector as it would be passing through the centre of the ring in such direction, that the robot's right side (with robotic arm) would be right next to the wall to which the ring is attached.
	
	\subsection{Task solving}
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[->, >=stealth', shorten >=1pt, auto, node distance=5cm, scale=0.6, 
		transform shape, align=center, state/.style={circle, draw, minimum size=2.3cm}]
		
			\tikzset{every edge/.append style={font=\footnotesize}}
			\tikzset{every node/.append style={font=\footnotesize}}
			
			\node[initial,state] 	(FG) {Finding \\ Gargamel};
	 		\node[state] (AG) 		[right of=FG] {Approaching \\ Gargamel};
  			\node[state]         	(FW) [right of=AG] {Finding \\ woman};
  			\node[state]         	(AW) [right of=FW] {Approaching \\ woman};
  			\node[state]         	(AG2) [right of=AW] {Approaching \\ Gargamel 2};
  			\node[state]         	(FC) [below of=AG2] {Finding \\ cylinder};
  			\node[state]         	(AC) [left of=FC] {Approaching \\ cylinder};
  			\node[state]         	(FR) [left of=AC] {Finding \\ ring};
  			\node[state]         	(AR) [left of=FR] {Approaching \\ ring};
  			\node[accepting, state] (AW2) [left of=AR] {Approaching \\ woman 2};
  			
  			\path (FG) edge[bend left] node {Gargamel \\ found} (AG);
  			
  			\path (AG) edge[bend left] node {Preferences \\ received} (FW);
  			\path (FW) edge[bend left] node {Woman according to \\ preferences found} (AW);
  			\path (AW) edge[bend left] node {Favorite colour received} (AG2);
  			\path (AG2) edge[bend left] node {Gargamel doesn't like her} (FW);
  			
  			\path (AG2) edge[bend left] node {Gargamel likes her} (FC);
  			\path (FC) edge[bend left] node {Cylinder \\ found} (AC);
  			\path (AC) edge[bend left] node {Cylinder \\ approached} (FR);
  			\path (FR) edge[bend left] node {Ring \\ found} (AR);
  			\path (AR) edge[bend left] node {Ring \\ approached} (AW2);
  			
  			\path (AW2) edge[] node {Woman not willing \\ to marry Gargamel} (FW);
  			
		\end{tikzpicture}
		\caption{Finite state automata}
		\label{fig:finite_state_automata}
	\end{figure}
	
	\section{Implementation and integration}
	In this section, the implementation of the algorithms explained in section \ref{methods} will be discussed. To solve our task, the following architecture was designed. We tried to keep the entire project as modular as possible, not only because this is the \texttt{ROS} operating system's philosophy, but also because three different people needed to work on the same project at the same time. \\ % tale zadnji stavek bi se dalo še mal boljš ubesedit
	
	This proved to be very difficult because nodes are so interconnected, that if any part of the system didn't work as intended, it seemed as if nothing worked. Some parts of the system were discussed beforehand, but as the tasks became more difficult, the arhitecture had to be changed in order to solve them. \\ 
	
	\begin{figure}[h]
		\centering
		\caption{Final architecture}
		\label{fig:final_architecture}
	\end{figure}
	
	\subsection{Object detectors}
	There are three different nodes responsible for object detections. Face and ring detectors are using \texttt{rgb} and depth images and the cylinder detector is using point cloud and \texttt{rgb} image to detect object in our world. \\
	
	All three object detectors are publishing a custom \texttt{ObjectDetection} message type. The \texttt{Robustifier} component subscribes to this type of message and tries to minimize the number of false positives. The robustification process is also the reason why we felt the need for and created the \texttt{ObjectDetection} message type.
	
	\subsubsection{ObjectDetection message}
	The object detection message contains the following information
	\begin{enumerate}
		\item \texttt{Header header}, which contains the timestamp of the detection and other metadata
		\item \texttt{string type}, which represents the type of the detected object (it can be either \texttt{"face"}, \texttt{"ring"} or \texttt{"cylinder"})
		\item \texttt{Pose object\_pose}, which represents the object position in world coordinate frame
		\item \texttt{Pose approaching\_point\_pose}, which is a point that the robot has to visit in order to complete the task
		\item \texttt{Image image}, which is an image of the object
		\item \texttt{ColorRGBA color} color and \texttt{string classified\_color} which represent the object average colour in \texttt{RGBA} format and the colour label that the colour classifier computed
	\end{enumerate}
	
	\subsubsection{Face detector node}
	The face detector is responsible for detecting and localizing faces. To do that, it fist synchronizes received \texttt{rgb} and depth images using the \texttt{Time Synchronizer} class in \texttt{message\_filters} package. \\

	After both, the \texttt{rgb} and depth images are received, it uses the Haar cascade algorithm explained in section \ref{face_detection_algorithm} to detect faces. If face has been found, it tries to compute its world position. This is done using raycasting from camera center, through the face center pixel in the image plane as explained in section \ref{pixel_to_world}. \\

	After face is localized, \texttt{Approaching point} is computed. \texttt{Approaching point} is a point that is positioned directly in front of the face. Because the faces are positioned on the walls, the closest wall to the face is found. Then, the wall normal is computed as explained in section \ref{face_orientation_computation}. The \texttt{Approaching point} is positioned 0.6 metres from the face position in the direction of the face orientation. \\

	\subsubsection{Ring detector node}
	Ring detector is a node that is responsible for ring detection and localization. It works similarly to the face detector node. First, it synchronizes depth and \texttt{rgb} images. Then it uses the ring detection algorithm as explained in section \ref{ring_detection}. Ring localization is done using the algorithm explained in section \ref{pixel_to_world}. \\
	
	Ring orientation is determined using similar algorithm as explained in section \ref{face_orientation_computation}, but instead of finding the normal of the wall, its direction is chosen as ring orientation. \\ % tu bi blo mogoč boljše vseen še enkrat povzet, kk deluje, ker če ne more bralec it pogledat nazaj gor, kje zamenjamo to normalo
	
	\subsubsection{Cylinder detector node}

	\subsection{Robustifier}
	For each object detector, a new \texttt{Robustifier} node is created. So each \texttt{Robustifier} node is listening for different object detections. The detections are then grouped together using the algorithm as explained in section \ref{robustification}. When number of detections sent to the \texttt{Robustifier} node exceeds the \texttt{threshold} value, the detection is considered a true positive and is sent to the \texttt{Brain} node. \\
	
	\subsection{Brain}
	\texttt{Brain} is a node that is responsible for solving the main part of the task. It subscribes to robustified object detections and uses them to solve the task. \\
	% Main part of the task?
	
	% Manjka, ker se ne vemo kako bo delalo
	
	\subsection{Movement controller}
	\texttt{Movement controller} is an object, that allows our robot to interact with its environment. It allows it to move and explore the space and use its robotic arm to grab items. It works using a series of tasks, which the robot executes one by one. When the task is finished, a callback function is used to notify the robot that the action is done. \\

	\subsubsection{Movement tasks}
	Each task represents one action that the robot can do.

	\begin{enumerate}
		\item \texttt{Localization task} is a task that is run at the beginning. The robot knows its approximate location in space, but before it can move around the environment, it must pinpoint its exact location. This task rotates the robot around its vertical axis for one full rotation using odometry information. The robot uses depth information to localize itself using adaptive Monte Carlo localization algorithm.
		\item \texttt{Approaching task} is a task that drives the robout around its environment. There are two options of this task, one is the standard \texttt{move\_base} package to drive around the map and the other uses our fine movement algorithm as explained in section \ref{fine_movement}. Only the rings are approached using the second algorithm, because it is rather slow (for the obvious reason - so that it doesn't hit the wall).
		\item \texttt{Wandering rask} is a task where the robot is exploring its environment, searching for faces, rings and cylinders. It uses the points generated using the algorithm in section \ref{smart_exploration} and drives the robot around using \texttt{approaching task}. This task can be cancelled at any time so the robot can visit detected objects if needed.
		\item \texttt{Arm task} is a task where the robot uses its pincher arm that is positioned on top of it. This task is used to throw a coin in a wishing well (moving the robot arm above a cylinder) or pick up a ring that is floating above the ground.
	\end{enumerate}

	The \texttt{movement controller} uses an ordered list of tasks. If the \texttt{brain} component chooses, it can cancel current task, reorder its task and insert new task to be executed either after all tasks have finished or right at this moment. \\ 
	
	\subsection{Greeter}
	Greeter is a node that uses \texttt{SoundClient} for speech syntesis. It is used whenever we want robot to say something out loud
	
	\section{Results}
	\section{Division of work}
	Since this project is extensive, we tried to build it from several modules to make it more manageable. Despite this, it was impossible to just divide the work exactly by modules as they are still very interconnected. It is also more convenient and fast to fix some error or slightly change an algorithm by yourself as you notice the need for it than explaining everything to the person who initially built the node and wait for them to make the changes the next time they sit at their computer. For these reasons the division of the work is not very clean and straightforward. Below we state which team member did most of the work for each of the robot's abilities.
	% We should only write what each of us did here and add that the professor should compute percentages himself.

	% Do we write a list of tasks and who did them or do we write a list of teammembers and what they did?
	% -------------------
	% cylinder detection: Martin
	% face detection: Niki
	% approaching points for rings: Ajda
	% ...
	% --------------------
	% OR
	% --------------------
	% Niki: face detection, bla bla bla
	% Martin: cylinder detection, bla bla bla
	% Ajda: approaching points for rings, ...
	% --------------------
	
	\section{Conclusion}
	After countless hours, Erazem was finnaly able to perform the task. We encoutered a bunch of problems on the way, the biggest one being the task of aprroaching the rings. We spent at least a week trying to reliably set the approaching points for the rings, wrote many (four i think) different algorithms (and additional variations of them) for this task, tried improving ring detection in several ways, change how the fine movement works multiple times, set up various robustification processes for both detections and approaching points, and even even setup a two layer robustification process. But all that with no definite sucess. In the presentation of the last task our robot did not set the points correctly for all of the rings and consequently couldn't approach some of them correctly. \\

	A big burden was also the fact that the presentation of the final task was right at the beginning of the exam period. This meant we had to "choose" between working on the final task and studying for the exams that took place at the beginning of the exam period. This was extremely stressfull. We prepared the architecture for the final task right after the presentations for the task 2 were held and also started working on the final task early, as soon as it was announced. Yet some development still had to be done it the last week befor the final task presentation. \\
	
	This course was very(the most?) time intensive. Not just becuse a lot of things had to be implemented but also beacuse they sometimes required a lot of research as ROS doesn't have everything well documented and it's difficult to find example usage for some modules. \\

	We are good friends and we worked well together as a team which definitely saved us some stress and some additional hours of work. It was good that we discussed changes before implementing them so we could see the changes and their impacts form different viewpoints and in their full extent. This also helped us foreseeing some preoblems which would be difficult to resolve otherwise without knowing the cause. \\

	Final thought: We wish ROS had better documentation or at least a debugger.

	% tasks should be published eralier so we could start earlier if we wanted to, same goes for the materials from excercises, we had to wait for qr codes, had to wait for some other components as well

	% the fun part was that the robot was speaking
	
\end{document}