\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage[bottom]{footmisc}

\author{Team Gamma \\ {\small Ajda Frankovič, Martin Preradović, Niki Bizjak}}
\title{Final report}
\date{}
\begin{document}
	
	\maketitle
	
	\section{Introduction}
	In this report we will explain how our robot by the name of Erazem performs the task of a skilled match maker. He was hired by Gargamel, who wanted to find love and get married as quick as possible. Erazem first had to find Gargamel to ask him about his preferences. He already knew that Gargamel was looking for a woman, but since usualy his clients also cared about the looks, he also wanted to find out about this kind of preferences. \\
	
	When Gargamel suprised him with his simple answer, that he only cared about the hair colour and hairstyle, he couldn't help but think to himself, what a strange man Gargamel was. He shook of this thought since he had had much stranger clients before and started looking right away. \\

	He was scanning the place for the women. For each face he saw, he quickly filled his mental checklist with Gargamel preferences. "Such easy criteria", he thought to himself, "but so little matches..." When he finally came across a woman that matched Gargamel's preferences, he fixed his tie, took a deep breath, and approached her. She seemed interested in meeting Gargamel but was very secretive. When Erasem asked her to tell him something about herself, she only told him her favourite colour. Erazem firmly remembered this little detail and after some chit-chat said goodbye to her. He was happy to have finally found a potential suitor but at the same time a little worried about the lack of information he had about her. \\
	
	Erazem thought it is time to report to Gargamel about his findings. They met at Gargamel's and Erasem told him all he knew about that woman. It was up to Gargamel to decide wheteher she seemed promising or no. If the Gargamel was to decline this suitor, Erazem would have gone out again and found him another one. When Gargamel liked the suitor Erazem suggested, he told him a bit nervously that he believed that tossing a coin into the wishing well and saying a wish makes that wish come true. Excellent matchmaker as Erazem was, he immediately understood. He agreed to find a wishing well in the woman's favourite color and toss a coin in it while also saying the wish for Gargamel and the woman to marrs and live happily ever after. \\

	Erazem searched through his memory whether he has already seen the well in this colour. If he could remeber, he would have definitely known where to go. After some time, he found the right wishing well, tossed the coin and said the wish. Now he went about to find a perfect ring. Of course, it had to be in the woman's favourite colour since Gargamel didn't have musch opinion on which colours he liked. He picked it up and brought it to the woman. \\
	
	This was the moment of truth. Will she agree to mary Gargamel, having received such thoughtfull gift? If she agrees, Erazem's work here is done. But what if she isn't impressed? Well, then Erazem will find another potential suitor and charge Gargamel some extra money, for the inconvenience. \\

	He knew that his job was very impactfull. He promised himself not to let Gargamel down. And the reward at the end will be beautiful. Two lost souls finding each other and living their happy ever after.

	% shema stanj robota
	
	\section{Methods} \label{methods}
	In this section, the algorithms and methods we have used in our system are presented.

	% Dodaj nek dictionary pojmov
	% To lahk anardimo v vsaki sekciji, kjer je smiselno, al pa sam na začetku. Sam po mojem si je lažje zapomnat in je manj overwhelming, če je mal pa po mal, torej v vsaki sekciji sproti, kar se rabi.
	% Izraze, ki jih definiramo tekom pisanja, lahka pišemo znotri \texttt{}, torej "...orientation of the face (from now on, \texttt{face orientation})..."
	% kaj je camera frame
	% kaj je camera coordinate system
	% kaj je world coordinate system
	% kaj je world frame
	% to se mogoče še najlepše razloži kr s slikico
	% world coordinates
	% map coordinates
	
	\subsection{Camera pixel to world position} \label{pixel_to_world}
	Object detectors must be able to compute the positions of the detected objects in the world. To do that, \texttt{rgb} camera and depth images are used. Because the robot is moving, and the environment is changing very fast, we must make sure that the received colour and depth image are synchronized with one another. Even a small delay between images can cause computed world positions to be inaccurate. \\ 
	
	%Dopiši, kaj smo uporabli za sinhronizacijo

	% dodaj sliko slabe detekcje zaradi pomanjkljive časovne sinhronizacije
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm]{images/detections}
		\caption{Inacurate detection of the object's position due to lack of time synchronization among depth and colour image}
		\label{fig:non_synchronized_raw_detection}
	\end{figure}
	
	Object detections are performed on images. Object detectors output pixels from \texttt{rgb} image (or in case of ring detector from depth image) where the object is detected. Using camera calibration matrix, focal length and image format can be obtained. Focal length is the distance from the centre of camera coordinate system to image plane. The image format gives us information about the width and height of the camera sensor. \\
	% Object detections are performed on WHAT KIND OF images?

	\begin{figure}[h]
		\centering
		\caption{Pinhole camera model}
		\label{fig:pinhole_camera_model}
	\end{figure}
	
	% Damo neko podobno sliko kot na naslednji povezavi
	% https://alicevision.readthedocs.io/en/latest/_images/pinholeCamera.png
	
	Assuming that the camera is using pinhole camera model (and the camera calibration matrix is known), a ray can be "cast" from the centre of the camera coordinate system to the pixel on image plane, where the object was detected. This can be seen in the figure \ref{fig:pinhole_camera_model}. Using depth image information, we can obtain the distance from the centre of the camera to the detected object in space. Using the computed ray and the distance to the object, we can compute object's position in a three-dimensional space in camera frame. \\
	% razloži, kaj je tu camera frame
	
	After that, we can use a transforamtion matrix to finally convert the position of the detected object from the camera frame to the world coordinate system. \\ 
	% dodaj te frame v nek dictionary (glej zgoraj)
	
	In our system, the face and ring positions are computed this way, meanwhile the detection of cylinders works with point clouds, where the points are already computed in the world frame.
	% kaj je world frame? Dodaj v dictionary
	
	\subsection{Faces}
	In this section, we present algorithms that our robot uses to detect faces, compute their orientation in space and classify them.
	
	\subsubsection{Face detection} \label{face_detection_algorithm}
	
	Face detection was done using Haar cascade face detection algorithm. We chose this algorithm because it can be run in real-time and it has a high detection rate. \\
	
	\begin{figure}[h]
		\centering
		\caption{A few examples of haar features}
		\label{fig:haar_features}
	\end{figure}
	
	The face images are first cropped to the same size and aligned so that the eyes approximately match. Then, a set of Haar features is generated. Each feature is defined at certain position in the face rectangle and consists of black and white regions (see figure \ref{fig:haar_features}). The value of the feature is computed as as difference between the sum of pixel intensities in the dark regions and the sum of pixel intensities in the light regions. \\
	
	Then, for each feature, a simple and fast binary classifier is trained on the training data that can recognize if it is looking at a face or not. Not all features prove to be very good at this task, so the features are ranked by their classification accuracy. With a boosting machine learning algorithm, many weak classifiers (as described above) are used to improve face detection accuracy. A cascade of weak detectors is created such that we start with the most accurate classifier and continue with less accurate ones. This gives the haar cascade algorithm its real-time ability. For some regions, the first few classifiers detect that there is definitely not a face and the detection can stop. This way, the most processing power is given to the areas that most likely contain a face. \\
	
	The detection is then done with a sliding window method. This simply means that we are evaluating every possible rectangle area in the image. If we want to detect faces of different scales, the image must be resized and the entire algorithm is repeated. So the cascade is crucial for speed here. \\
	
	\subsubsection{Computing the orientation of the faces} \label{face_orientation_computation}
	After the face has been detected in an image, it's position in the world coordinate system is computed. The approaching point is then calculated using static map information. \texttt{Approaching point} is a point close to the face and directly in front it that the robot must visit to approach the face. \\
	% Evo, tu je en primer definicije novega izraza (approaching point)
	
	To compute the \texttt{approaching point}, we must first find the orientation of the detected face (from now on \texttt{face orientation}). In order to do that, we need the static map information.
	% what static map information?
	The calculated position of the detected face (from now on \texttt{face position}) in the world coordinates is first converted to map coordinates. Then, the corresponding pixel in map is calculated. \\
	% dodaj world coordinates v dictionary
	% dodaj map coordinates v dictionary
	
	\begin{figure}[h]
		% This image should be replaced with something better
		\centering
		\begin{tikzpicture}
			\draw[step=0.5cm,lightgray,very thin] (0.1,1.1) grid (6.4,4.9);
			\fill[darkgray] (0,3.5) rectangle (6.5,4);
			\fill[red] (3,3.5) rectangle (3.5,4);
			\fill[red] (3.25,1.75) circle (0.15);
			\node[] at (4,2) {$(x, y)$};
			\draw [->] (1.75,3.75) to (1.75,2.75) node [right] {$\vec{n}$};
			\draw [->] (1.75,3.75) to (1.75,4.75) node [right] {$-\vec{n}$};
			\draw[very thick, black] (-0.5,3.75) -- (7,3.75);
		\end{tikzpicture}
		\caption{Face orientation computation}
		\label{fig:orientation_computation}
	\end{figure}
	
	Because of depth sensor inaccuracy, the computed map pixel coordinate doesn't always lie on the wall. In figure \ref{fig:orientation_computation}, computed face pixel is coloured red. \\ 
	% map pixel coordinate v dictionary
	
	% TO NI VEČ RELEVANTNO, KER JE ZDJ DRUGA SLIKA
	Using a simple breadth first search in the proximity of the pixel which corresponds to the position of the detected face (from now on \texttt{face pixel}), the closest pixel from the set of pixels, which correspond to the positions occupied by the walls (from now on \texttt{wall pixel}), is determined. In figure \ref{fig:orientation_computation}, this pixel is also coloured red. \\
	% Damn it, ne morta bit oba rdeča. Če jih hočemo opisat z barvami, morta bit različnih barv. Sicer ju mormo opisat drugače, recimo dodat oznaki A in B
	
	% TO NI VEČ RELEVANTNO, KER JE ZDJ DRUGA SLIKA
	After the wall pixel has been found, the Hough line finding algorithm is executed on an area around it. The detected lines in figure \ref{fig:orientation_computation} are represented with a yellow colour. If the algorithm finds more than one line, the line which is closest to the wall pixel is selected. After the line is selected, there are two possible orientations as represented in figure \ref{fig:orientation_computation}. The vector that is pointing in the direction of the robot is chosen to be the face orientation. \\ 
	% prestrukturiraj v smislu da normala na najdeno linijo nam določa orientacijo obraza. Ker pa mamo vektorje, sta možni orientaciji dve, vsaka v svoji smeri (v nasprotnih smereh). Ker robot ne vidi skozi zid, pride v poštev tista izmed orientacij, ki je bližje vektorju od pozicije obraza do pozicije robota.
	
	\subsubsection{Face classification}
	After faces are detected and localized in the world, they have to be identified.
	
	\subsection{Colour classification} \label{colour_classification}
	The robot must be able to detect the colour of the rings and cylinders. There are six possible classes: black, white, red, green, blue and yellow. \\
	
	In homework 2, we tested different colour spaces and classification models and concluded that the k-nearest neighbours algorithm worked best. The colour space with the highest accuracy in homework was \texttt{HSV} colour model, but in the Gazebo simulator, \texttt{RGB} space worked better. \\
	
	So the colour classifier in the final task uses the k-nearest neighbours algorithm and takes in an input vector in $(red, green, blue)$ format and returns a colour label with one of the six possible classes listed above. \\
	
	Because of the uneven lighting in the Gazebo simulator (and probably in the real world too), the classifier sometimes returns an incorrect label. We solved this problem by using the robustification process as described in the \ref{robustification} section. Colour classifier is run multiple times on different detections of the same object and the most frequent colour is chosen as the colour of the object (from now on \texttt{object colour}). \\
	
	\subsection{Rings}
	In this section, the algorithm for ring detection is presented.
	
	\subsubsection{Ring detection} \label{ring_detection}
	When robot finds the woman that is willing to marry Gargamel, it must help him find a ring that she will like. Luckily, she is kind enough to tell us her favourite colour. The robot must then find the ring in that colour, give it to her and ask her to marry him. \\
	
	But before the ring can be picked up, it must first be detected by the robot. The ring detection is performed on the depth image and the ring colour detection is performed on the \texttt{rgb} image. Again, both images must be synchronized, so it can be accurately localized. \\
	
	% I am not sure that the information below is correct
	Depth image is computed from disparity image, which is calculated from Kinect stereo camera system. The further away an object is, the smaller its disparity will be. Using calibration matrix, depth of each pixel can be approximated from disparity image. Object that appear further away from the camera have a smaller disparity, which reduces the accuracy of depth computation. To combat this, our algorithm first removes all objects that are over a certain distance away from the camera. \\
	
	After inaccurate depths are removed, a blob detection algorithm is applied to our depth image. The algorithm finds regions in our image that differ in colour. It actually searches for dark areas (areas that are further away from the camera) in the depth image. Because the inside of the ring is darker than the ring itself, the inner part is considered a blob. But not all blobs are considered rings. We can then apply some domain knowledge to the problem. We know that all the rings are positioned 11 cm above the cubes, so we can filter all blobs that lie below that height. If we look at the ring from any angle, they look elliptical, so the blobs are filtered by their roundness too. \\
	
	\begin{figure}[h]
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.8\linewidth]{images/ring_detection}
			\caption{Ring detection}
			\label{fig:detected_ring}
		\end{subfigure}
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.8\linewidth]{images/ring_detection_cropped}
			\caption{Cropped ring detection}
			\label{fig:detected_ring_cropped}
		\end{subfigure}
		\caption{Ring detection using blob detector}
		\label{fig:ring_detection}
	\end{figure}

	In figure \ref{fig:detected_ring}, we can see an example of detected ring. Even though the corner of the cube is overlapping with the circle, the ring is still detected. This also presents another problem. To localize the ring, we must know the distance to the ring, but which distance should we use? \\
	
	To compute the distance from camera to the ring, a histogram of distances is created. In figure \ref{fig:distance_histogram}, we can see an example histogram for the figure \ref{fig:detected_ring_cropped} with 24 bins. As we can see, there are two bars that stand out. One of them is the edge of the cube and the other is the ring. A mask is constructed so that only distances that lie in the highest bucket are retained, everything else is removed. \\
	
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}
			\begin{axis}[
			ybar,
			xlabel={Distance}, ylabel={Number of pixels},
			xmin=0, xmax=23, ymin=0, ymax=1400,
			xtick={0,4,8,12,16,20,24},
			ytick={0,200,400,600,800,1000,1200,1400,1600},
			width=12cm, height=5cm
			]
			\addplot[fill=gray]
			coordinates {
				(0, 0) (1, 0) (2, 0) (3, 0) (4, 0) (5, 0) (6, 0) (7, 0) (8, 306) (9, 0) (10, 1261) (11, 0) (12, 0) (13, 0) (14, 0) (15, 0) (16, 0) (17, 0) (18, 0) (19, 0) (20, 0) (21, 0) (22, 0) (23, 0) 
			};
			\end{axis}
		\end{tikzpicture}
		\caption{Distance histogram}
		\label{fig:distance_histogram}
	\end{figure}

	The mask is then applied to both, the distance image and the colour image. In figure \ref{fig:masked_colour_ring}, we can see the result of filtering of the \texttt{rgb} colour image using the mask obtained in the previous step. \\

	The distance to the ring is computed as the average of distances to each pixel in the masked distance image. The ring position in world frame is computed using as explained in section \ref{pixel_to_world}. \\

	\subsubsection{Ring approaching point computation}
	Computation of approachin points for rings takes advantage of the static map. First we check whether the detected ring position is above the wall or not. If it is, we use it's position as our starting position. If it isn't, we find the closest pixel occupied by the wall and use it as our starting position. Next, we need to find the direction in which we'll set the approaching point. \\

	We do this by searching for the nearest pixel to the starting poisiton (located on the wall) that is not occupied by a wall. By subtracting the starting position pixel from the pixel we just found, we get a vector that points from the ring to our soon to be approaching point. This vector is then normalized so it can be used in further computation. \\

	Now we move along the calculated direction vector for 0.15 meters and set this position as the position of the approaching point. Orientation for the approaching point is obtained by rotating the direction vector 90 degrees to the right. This sets the orientation vector as it would be passing through the centre of the ring in such direction, that the robot's right side (with robotic arm) would be right next to the wall to which the ring is attached.

	\subsubsection{Ring colour detection}
	The ring colour is computed using the mask that we have computed in section \ref{ring_detection}. The \texttt{rgb} image is filtered so that only ring is left and then the colour is averaged. The average colour is then classified using the algorithm explained in section \ref{colour_classification}.
	
	\begin{figure}[h]
		\centering
		\includegraphics[height=5cm]{images/ring_detection_colour}
		\caption{Masked colour ring}
		\label{fig:masked_colour_ring}
	\end{figure}
		
	\subsection{Cylinder detection}
	Cylinder detection runs on a point cloud data that is computed from depth information. \\
	
	\subsubsection{Removal of planes}
	\subsubsection{Cylinder detection}
	
	\subsubsection{Cylinder approaching point computation}
	The cylinder approaching point computation is not as difficult as the one for faces. If cylinder has been detected from current robot position, that means that the robot must have a clear view of the cylinder. Because of that, the approaching point can be computed as a point on the line connecting the robot and the cylinder. \\
	
	A point that lies 0.5 metres from the centroid of the cylinder on this line is selected as the approaching point. If the point can't be reached (that is, if it is too close to the wall), approaching point is repositioned so that there is enough space for the robot to visit the point. This is done by finding the closest wall on the map and then moving the point in the opposite direction. This is repeated until there is no wall too close to the point. \\
	
	\subsection{Robustification} \label{robustification}
	The sensors are not 100\% accurate, so we have to take into account some deviation. For example, when an object is detected, the depth sensor might compute the distance inaccurately, or the robot may not have been localized. Detectors might return false positives and so on. \\
	
	To combat this problem, a robustification is performed on the detected objects. The object detectors send detections to robustifier, which collects data and determines if the detection is a true positive or not. \\
	
	For detection to be considered a true positive, the following must hold true:
	\begin{enumerate}
		\item Detected object was detected at least \texttt{threshold} times
		\item There is at least a \texttt{minimum distance} meters distance between this detection and every other previous detection.
	\end{enumerate}

	If two detections are close enough (the distance between them is less than \texttt{minimum distance}), they are considered the same object. The positions of those two detections in the world are then averaged, so the object position is closer to its actual position in space.
	
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm]{images/detections}
		\caption{Many inaccurate object detections}
		\label{fig:inaccurate_detections}
	\end{figure}

	Figure \ref{fig:inaccurate_detections} shows why the robustification process is needed. Face detector works very fast, but the localization isn't that accurate. Each red sphere represents one face detection. By averaging many face detections, the pink sphere position is obtained. As we can see, the the averaged position is much closer to the real face position (which is in this case on the wall near this averaged position) than each individual detection. The distance between the farthest points in figure \ref{fig:inaccurate_detections} is 0.3 metres, which is another reason why we think robustification process is needed. \\
	
	It should also be noted that similar process is applied to the approaching points. Approaching point is computed for each face detection and as positions of the face detections are averaged, approaching points are averaged as well. In figure \ref{fig:inaccurate_detections}, this point is yellow. \\
		
	\subsection{Smart exploration of space} \label{smart_exploration}
	There are two ways to determine some goals for space exploration; either you hardcode a list of goals or you create a system that automatically sets the goals based on the shape of the map. Hardcoding is almost never a good idea, se we went with the latter. \\

	Our algorithm uses static map as the input. First, the contrast between the walls or unknown space, and the space where robot can move, is amplified. After that a morphological operation of closing is performed to smooth out the corners which robot can't reach because of it's circular shape. \\
	
	The part containing the relevant map is cropped out of the whole image as the other empty parts are not needed in the rest of the process. We then find the corners with \texttt{Harris corner detector} and then apply the \texttt{non maxima suppresion} to only keep the actual corners. The corners are corners in the walls and obstacles and also the ends of the walls. \\
	
	An edge is a sudden change in brightness and a corner is the point where two edges meet. Corners are the points, where gradient change is big in all directions. Harris corner detector works on a grayscale image, smoothed by a \texttt{Gaussian filter} (removes any noise). For each pixel in the image, it considers a 3x3 window around it and uses derivatives to computes a \texttt{Harris value} for it. \texttt{Harris value} is a score which tells how likely does the pixel contain a corner. \texttt{Non maxima suppression} is used to only declare corners in the pixels, whose score is the local maximum of the certain window and also exceed some threshold value. \\

	With the list of corners in the map ready, we can perform \texttt{Delaunay triangulation} to split the space without obstacles into triangles. \texttt{Delaunay triangulation} is a special kind of triangulation, where  no point form the set that the triangulation is performed on is inside the circumcircle of any triangle in the trinagulation. Another property of the \texttt{Delaunay triangulation} is that it maximizes the minimum angle in the triangles and thus create more even distribution of the area. \\

	For each triangle, our algorithm find its centroid - the center of the circumcircle. If we were to connect the centroids, we would obtain a so called Voronoi diagram, a partition of the plane into regions. Each region contains all of the points that are closer to the seed inside this region, in our case a corner, than to any other seed (corner). The points from the \texttt{Delaunay triangulation} (centroids) are therefore as far away from all walls as possible. This makes them great points for space exploration as they are far from the obstacles and somewhat evenly distributed across the map. \\

	We have to be careful as centroids form the \texttt{Delaunay triangulation} will also be set in narrow passageways to which robot may not have access. And sometimes, two points might end up very close together, which isn't ideal for the exploration. \\
	
	To eliminate the second problem, we use \texttt{hierarhical clustering} with the minimum distance as the stopping criteria. This merges any points that are too close together into one, their average. It does so hierarhically, in each iteration it merges only the two points that are the closest. \\
	
	This step can sometimes also solve the first problem. To be sure that the robot can reach all of the exploration goals, we check if any goal is too close to any wall and if so, move it into the opposite direction, away from the wall, if possible. \\

	In the figure below, we can see the \texttt{Delaunay triangulation} of our map in green and the centroids in red. The points in blue represent the exploration goals after \texttt{hierarhical clustering} and moving points away from the walls.
	% slikici before - after clustering (before naj ma še trikotnike, da se vidi triangualcija - lahka je pa tud posebi slikica sam za trikotnike)

	\begin{figure}[h]
		\centering
		\caption{\texttt{Delaunay triangulation} and exploration goals before (left) and after the \texttt{hierarhical clustering} and moving unreachable ones to reachable position (right)}
		\label{fig:exploration_goals}
	\end{figure}
	
	\subsection{Fine movement} \label{fine_movement}
	Fine movement is a special way in which our robot can move. It is used when the default movement wouldn't be able to get us to the goal even though the goal is reachable.  \\

	The \texttt{move\_base} package can be used with \texttt{ROS} to move the mobile base around the map. It uses local and global planner information to avoid obstacles and try to reach the given goal. If it fails to reach the goal, it tries to clear its local map. If that fails, the robot stops trying to reach the goal and notifies us that it has failed to move to the goal. \\
	
	The most common reason for failure is that the goal is too close to any wall in our world. The robot builds a costmap of its environment, where the closer the position is to the wall, the highest cost it has. This way a robot can avoid obstacles and plan the path from point A to point B. The robot builds a costmap in a way that it makes sure not to bump into anything, which means it also takes into account all possible errors in data from its detectors. This means that some of the points that the robot can reach in reality unfortunately fall into the high-cost area and so the normal movement cannot get us there because the risk of bumping into something is too high. \\

	In our final task, the robot had to be able to pick up rings that are positioned very close to the wall. So close actually, that the default movement component is unable to reach the goal. To do that, odometry data must be used and combined with \texttt{Twist} messages to move the robot manually. \\
	
	Because the robot must be able to visit the ring from specific direction (ring orientation), we used a modified version of \texttt{A*} algorithm, that not only finds the shortest path between two points, but also uses the starting and ending robot orientation. \\
	
	Because the world is flat\footnote{Does this make us flat-earthers? Nah. Does this make our robot a flat-earther? Maybe.} and the robot can only rotate along one axis, any robot pose can be represented using a tuple $(x, y, \varphi)$, where $\varphi$ is the orientation along the vertical axis. In theory, the $\varphi$ can be any real number, but in our case, the robot will only have a finite number of possible orientations (the first one being $[0, \frac{2\pi}{n}]$, where $n$ is the number of all orientations). In our final task, we found that $n = 8$ or $n = 16$ works best. \\
	% končno število rotacij zato, da je problem rešljiv. Rešitev se da najt tudi samo z 8 ali 16 različnimi orientacijami. Treba je še dodati, da če imamo 8 možnih rotacij, mora A* kr naenkrat preiskat 8*height*width pixlov (trojic x,y,fi) in že to kr poveča kompleksnost.
	% n=8 pomeni 45-stopinjske obrate

	The algorithm will use map to access environment information, so the $x$ and $y$ represent pixel coordinate in map coordinate system. \\
	% razloži kaj dejansko pomenijo te (x, y) koordinate pod imenom pixli. V kaki relaciji so z globalnimi koordinatami/posi od robota?
	
	\begin{figure}[H]
		\centering
		
		\begin{tikzpicture}
		\draw[step=1cm,gray,very thin] (0.1,0.1) grid (3.9,3.9);
		\draw[thick] (1.5,1.5) circle (0.4cm);
		\draw [thick, ->] (1.78284271247, 1.78284271247) to (2.13639610307,2.13639610307) node [right] {};
		\node[] at (2.5,2.5) {1};
		\node[] at (1.5,2.5) {2};
		\node[] at (2.5,1.5) {3};
		\end{tikzpicture}
		\caption{Next possible robot states}
		\label{fig:next_robot_states}
	\end{figure}
	
	If the robot is currently in state $\vec{s_i} = (x_i, y_i, \varphi_i)$, we can define the next states to be:
	
	\begin{enumerate}
		\item Move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i)$$
		\item Rotate left by $\frac{2\pi}{n}$ and move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i - \frac{2\pi}{n})$$
		\item Rotate right by $\frac{2\pi}{n}$ and move forward
		$$\vec{s_{i+1}} = (x_i + \alpha \cos \varphi_i, y_i + \alpha \sin \varphi_i, \varphi_i + \frac{2\pi}{n})$$
	\end{enumerate}
	
	Where $\alpha$ is the step size (to move 1 pixel forward, $\alpha = 1.5$). Each state is only possible if the map does not contain a wall at pixel $(x_{i+1}, y_{i+1})$. Because the orientations are cyclical, if the robot orientation is less than $0$ or more than $2\pi$, the orientation is wrapped around to be inside $[0, 2\pi]$ range. \\
	% zakaj je alfa 1,5: dodaj slikico al pa razloži, da zato, ker če je 1, lahka gremo sam gor dol levo desno, ne pa tud diagonalno.
	% "Each state" ali "Each step"?
	% "not contain a wall at pixel $(x_{i+1}, y_{i+1})$" dodaj, da je to pixel, na kerga se želimo prestavit/na kerga nas pripelje korak
	
	The figure \ref{fig:next_robot_states} represents a robot at some coordinates $(x, y, \varphi)$. The robot is oriented in the direction of the arrow. In the example, the orientation is $\varphi = \frac{\pi}{4}$. The next possible states are numbered as defined above with numbers 1, 2 and 3 respectively. In this case, the number of possible orientations $n = 8$. \\
	
	The heuristic function can be defined as an Euclidean distance between current state and the end state. $$h(s_i) = \sqrt{(x_{end} - x_i)^2 + (y_{end} - y_i)^2 + (\varphi_{end} - \varphi_i)^2}$$. To avoid getting too close to the walls, a penalty is added to the pixels that are too close to the wall. \\
	
	The \texttt{A*} algorithm can now be run using states as defined above. We start with initial robot state $(x_0, y_0, \varphi_0)$ and set the goal to the ring position $(x_m, y_m, \varphi_m)$. The closest path between those two states is the path that the robot should follow to get from its current position to the goal position including its orientations. \\
	
	After the path is computed, the robot can subscribe to odometry data to get its position and follow the list of poses that the previous algorithm has computed. To move from one pose to another, we wait until next odometry message is received, then rotate until we are oriented in the direction to the next goal and then move until we are close enough to the next pose. \\
	
	\section{Implementation and integration}
	In this section, the implementation of the algorithms explained in section \ref{methods} will be discussed. To solve our task, the following architecture was designed. We tried to keep the entire project as modular as possible, not only because this is the \texttt{ROS} operating system's philosophy, but also because three different people needed to work on the same project at the same time. \\ % tale zadnji stavek bi se dalo še mal boljš ubesedit
	
	This proved to be very difficult because nodes are so interconnected, that if any part of the system didn't work as intended, it seemed as if nothing worked. Some parts of the system were discussed beforehand, but as the tasks became more difficult, the arhitecture had to be changed in order to solve them. \\ 
	
	\begin{figure}[h]
		\centering
		\caption{Final architecture}
		\label{fig:final_architecture}
	\end{figure}
	
	\subsection{Object detectors}
	There are three different nodes responsible for object detections. Face and ring detectors are using \texttt{rgb} and depth images and the cylinder detector is using point cloud and \texttt{rgb} image to detect object in our world. \\
	
	All three object detectors are publishing a custom \texttt{ObjectDetection} message type. The \texttt{Robustifier} component subscribes to this type of message and tries to minimize the number of false positives. 
	
	\subsubsection{ObjectDetection message}
	The object detection message contains the following information
	\begin{enumerate}
		\item \texttt{Header header}, which contains the timestamp of the detection and other metadata
		\item \texttt{string type}, which represents the type of the detected object (it can be either \texttt{"face"}, \texttt{"ring"} or \texttt{"cylinder"})
		\item \texttt{Pose object\_pose}, which represents the object position in world coordinate frame
		\item \texttt{Pose approaching\_point\_pose}, which is a point that the robot has to visit in order to complete the task
		\item \texttt{Image image}, which is an image of the object
		\item \texttt{ColorRGBA color} color and \texttt{string classified\_color} which represent the object average colour in \texttt{RGBA} format and the colour label that the colour classifier computed
	\end{enumerate}

	% Why have we decided that we needed object detection message. Because of our robustification process.
	
	\subsubsection{Face detector node}
	The face detector is responsible for detecting and localizing faces. To do that, it fist synchronizes received \texttt{rgb} and depth images using the \texttt{Time Synchronizer} class in \texttt{message\_filters} package. \\

	After both, the \texttt{rgb} and depth images are received, it uses the Haar cascade algorithm explained in section \ref{face_detection_algorithm} to detect faces. If face has been found, it tries to compute its world position. This is done using raycasting from camera center, through the face center pixel in the image plane as explained in section \ref{pixel_to_world}. \\

	After face is localized, \texttt{Approaching point} is computed. \texttt{Approaching point} is a point that is positioned directly in front of the face. Because the faces are positioned on the walls, the closest wall to the face is found. Then, the wall normal is computed as explained in section \ref{face_orientation_computation}. The \texttt{Approaching point} is positioned 0.5 metres from the face position in the direction of the face orientation. \\

	\subsubsection{Ring detector node}
	Ring detector is a node that is responsible for ring detection and localization. It works similarly to the face detector node. First, it synchronizes depth and \texttt{rgb} images. Then it uses the ring detection algorithm as explained in section \ref{ring_detection}. Ring localization is done using the algorithm explained in section \ref{pixel_to_world}. \\
	
	Ring orientation is determined using similar algorithm as explained in section \ref{face_orientation_computation}, but instead of finding the normal of the wall, its direction is chosen as ring orientation. \\ % tu bi blo mogoč boljše vseen še enkrat povzet, kk deluje, ker če ne more bralec it pogledat nazaj gor, kje zamenjamo to normalo
	
	\subsubsection{Cylinder detector node}

	\subsection{Robustifier}
	For each object detector, a new \texttt{Robustifier} node is created. So each \texttt{Robustifier} node is listening for different object detections. The detections are then grouped together using the algorithm as explained in section \ref{robustification}. When number of detections sent to the \texttt{Robustifier} node exceeds the \texttt{threshold} value, the detection is considered a true positive and is sent to the \texttt{Brain} node. \\
	
	\subsection{Brain}
	\texttt{Brain} is a node that is responsible for solving the main part of the task. It subscribes to robustified object detections and uses them to solve the task. \\
	% Main part of the task?
	
	% Manjka, ker se ne vemo kako bo delalo
	
	\subsection{Movement controller}
	\texttt{Movement controller} is an object, that allows our robot to interact with its environment. It allows it to move and explore the space and use its robotic arm to grab items. It works using a series of tasks, which the robot executes one by one. When the task is finished, a callback function is used to notify the robot that the action is done. \\

	\subsubsection{Movement tasks}
	Each task represents one action that the robot can do.

	\begin{enumerate}
		\item \texttt{Localization task} is a task that is run at the beginning. The robot knows its approximate location in space, but before it can move around the environment, it must pinpoint its exact location. This task rotates the robot around its vertical axis for one full rotation using odometry information. The robot uses depth information to localize itself using adaptive Monte Carlo localization algorithm.
		\item \texttt{Approaching task} is a task that drives the robout around its environment. There are two options of this task, one is the standard \texttt{move\_base} package to drive around the map and the other uses our fine movement algorithm as explained in section \ref{fine_movement}. Only the rings are approached using the second algorithm, because it is rather slow (for the obvious reason - so that it doesn't hit the wall).
		\item \texttt{Wandering rask} is a task where the robot is exploring its environment, searching for faces, rings and cylinders. It uses the points generated using the algorithm in section \ref{smart_exploration} and drives the robot around using \texttt{approaching task}. This task can be cancelled at any time so the robot can visit detected objects if needed.
		\item \texttt{Arm task} is a task where the robot uses its pincher arm that is positioned on top of it. This task is used to throw a coin in a wishing well (moving the robot arm above a cylinder) or pick up a ring that is floating above the ground.
	\end{enumerate}

	The \texttt{movement controller} uses an ordered list of tasks. If the \texttt{brain} component chooses, it can cancel current task, reorder its task and insert new task to be executed either after all tasks have finished or right at this moment. \\ 
	
	\subsection{Greeter}
	
	\section{Results}
	\section{Division of work}
	% We should only write what each of us did here and add that the professor should compute percentages himself.
	
	\section{Conclusion}	
	
\end{document}